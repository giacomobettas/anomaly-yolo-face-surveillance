{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO + Face ID + Autoencoder + FFT-Based Anomaly Scoring (Colab Demo)\n",
        "\n",
        "This notebook demonstrates the end-to-end pipeline:\n",
        "\n",
        "1. **Train** a per-camera convolutional autoencoder on **normal** person crops\n",
        "2. **Process** a video using:\n",
        "   - YOLO person detection\n",
        "   - optional face recognition identity\n",
        "   - AE reconstruction error\n",
        "   - FFT motion feature (center_y trajectory)\n",
        "   - combined anomaly score and flags\n",
        "3. Save outputs (models, checkpoints, CSV logs, annotated videos) to **Google Drive**.\n"
      ],
      "metadata": {
        "id": "tO3v_Lem7TAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "_664dHONAKOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Paths / Config"
      ],
      "metadata": {
        "id": "GgCc281lAPhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change this if you prefer a different Drive location\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/anomaly_yolo_face_surveillance\"\n",
        "\n",
        "REPO_DIR = \"/content/anomaly-yolo-face-surveillance\"\n",
        "\n",
        "DATA_DIR = os.path.join(DRIVE_BASE, \"data\")          # Your crops + faces\n",
        "VIDEOS_DIR = os.path.join(DRIVE_BASE, \"videos\")      # Input videos\n",
        "OUTPUTS_DIR = os.path.join(DRIVE_BASE, \"outputs\")    # Logs + annotated videos\n",
        "MODELS_DIR = os.path.join(DRIVE_BASE, \"models\")      # Saved .keras models\n",
        "CKPT_DIR = os.path.join(DRIVE_BASE, \"checkpoints\")   # Saved weights .h5\n",
        "\n",
        "for p in [DATA_DIR, VIDEOS_DIR, OUTPUTS_DIR, MODELS_DIR, CKPT_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"Drive base:\", DRIVE_BASE)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"VIDEOS_DIR:\", VIDEOS_DIR)\n",
        "print(\"OUTPUTS_DIR:\", OUTPUTS_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "print(\"CKPT_DIR:\", CKPT_DIR)\n"
      ],
      "metadata": {
        "id": "9JnA89jIAQKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Repo"
      ],
      "metadata": {
        "id": "QwLXKfiOAcY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {REPO_DIR}\n",
        "!git clone https://github.com/giacomobettas/anomaly-yolo-face-surveillance.git {REPO_DIR}\n",
        "%cd {REPO_DIR}\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "X_rE1CbkAd0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "c4Yp_tEJAhCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "dbwd_1ikAiTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: enable face recognition (face_recognition + dlib)\n",
        "\n",
        "Face recognition is **optional** in this repo.\n",
        "\n",
        "- If you skip it, identities will be `\"unknown\"`, but everything else works.\n",
        "- If you enable it, you need system packages + `dlib` compilation.\n"
      ],
      "metadata": {
        "id": "Cvqjc3S_AuGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: enable face_recognition in Colab (Linux)\n",
        "# This may take a while because it compiles dlib.\n",
        "\n",
        "!apt-get update -y\n",
        "!apt-get install -y cmake build-essential\n",
        "!pip install face_recognition\n"
      ],
      "metadata": {
        "id": "oCnpgOa2AxLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected data layout (stored in Drive)\n",
        "\n",
        "### Autoencoder training (normal crops per camera):\n",
        "`{DATA_DIR}/cam1/normal_crops/*.jpg`\n",
        "\n",
        "Example:\n",
        "- `.../data/cam1/normal_crops/img001.jpg`\n",
        "- `.../data/cam1/normal_crops/img002.jpg`\n",
        "\n",
        "### Optional face identities:\n",
        "`{DATA_DIR}/faces/<person_name>/*.jpg`\n",
        "\n",
        "Example:\n",
        "- `.../data/faces/person1/face1.jpg`\n",
        "- `.../data/faces/person2/face2.jpg`\n",
        "\n",
        "### Input videos:\n",
        "`{VIDEOS_DIR}/cam1_example.mp4`\n"
      ],
      "metadata": {
        "id": "zOg3QEH2Azto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Synthetic Crops Generator\n",
        "\n",
        "If you want the notebook to run “out of the box” without uploading data yet, this generates a tiny fake dataset (just for smoke-checking AE training)."
      ],
      "metadata": {
        "id": "Ukq2CDw_A7l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "cam_id = \"cam1\"\n",
        "normal_dir = os.path.join(DATA_DIR, cam_id, \"normal_crops\")\n",
        "os.makedirs(normal_dir, exist_ok=True)\n",
        "\n",
        "# Create a few synthetic \"person-like\" blobs\n",
        "H, W = 128, 64\n",
        "for i in range(40):\n",
        "    img = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "    cx = np.random.randint(W//3, 2*W//3)\n",
        "    cy = np.random.randint(H//3, 2*H//3)\n",
        "    cv2.ellipse(img, (cx, cy), (W//6, H//4), 0, 0, 360, (255, 255, 255), -1)\n",
        "    cv2.GaussianBlur(img, (5, 5), 0, dst=img)\n",
        "    cv2.imwrite(os.path.join(normal_dir, f\"synthetic_{i:03d}.jpg\"), img)\n",
        "\n",
        "print(\"Synthetic crops written to:\", normal_dir)\n",
        "print(\"Count:\", len(os.listdir(normal_dir)))\n"
      ],
      "metadata": {
        "id": "0gXhc5Z_BBUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the autoencoder (per camera)\n",
        "\n",
        "We train the AE on **normal** crops only.\n",
        "Outputs are saved to Drive:\n",
        "\n",
        "- Best weights: `{CKPT_DIR}/<cam_id>_ae_best.weights.h5`\n",
        "- Final model: `{MODELS_DIR}/<cam_id>_ae_person_autoencoder.keras`\n",
        "\n",
        "You can restart the Colab runtime and continue because everything is in Drive.\n"
      ],
      "metadata": {
        "id": "TIKmgUepBFe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Training"
      ],
      "metadata": {
        "id": "6gm17UXWBJqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cam_id = \"cam1\"\n",
        "\n",
        "train_dir = os.path.join(DATA_DIR, cam_id, \"normal_crops\")\n",
        "best_ckpt = os.path.join(CKPT_DIR, f\"{cam_id}_ae_best.weights.h5\")\n",
        "final_model = os.path.join(MODELS_DIR, f\"{cam_id}_ae_person_autoencoder.keras\")\n",
        "\n",
        "!python -m src.ae_train \\\n",
        "  --camera_id {cam_id} \\\n",
        "  --data_dir \"{train_dir}\" \\\n",
        "  --image_size 128 64 \\\n",
        "  --color_mode rgb \\\n",
        "  --batch_size 16 \\\n",
        "  --epochs 10 \\\n",
        "  --checkpoint_path \"{best_ckpt}\" \\\n",
        "  --model_path \"{final_model}\" \\\n",
        "  --patience 3\n"
      ],
      "metadata": {
        "id": "OiNC4YT-BK14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provide an input video\n",
        "\n",
        "Place a video in:\n",
        "`{VIDEOS_DIR}/cam1_example.mp4`\n",
        "\n",
        "If you don't have one, you can upload a short test video to Drive manually.\n",
        "Then update the filename below.\n"
      ],
      "metadata": {
        "id": "0Fu4tmzoBQfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Video Paths"
      ],
      "metadata": {
        "id": "yiOMM5vNBVaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cam_id = \"cam1\"\n",
        "\n",
        "video_path = os.path.join(VIDEOS_DIR, \"cam1_example.mp4\")  # change if needed\n",
        "ae_model_path = os.path.join(MODELS_DIR, f\"{cam_id}_ae_person_autoencoder.keras\")\n",
        "\n",
        "annotated_out = os.path.join(OUTPUTS_DIR, f\"{cam_id}_annotated.avi\")\n",
        "csv_out = os.path.join(OUTPUTS_DIR, f\"{cam_id}_log.csv\")\n",
        "\n",
        "print(\"video_path:\", video_path)\n",
        "print(\"ae_model_path:\", ae_model_path)\n",
        "print(\"annotated_out:\", annotated_out)\n",
        "print(\"csv_out:\", csv_out)\n"
      ],
      "metadata": {
        "id": "v_kUppOoBWwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the full pipeline\n",
        "\n",
        "This performs:\n",
        "- YOLO person detection\n",
        "- optional face identification (`--faces_root`)\n",
        "- AE reconstruction error per person crop\n",
        "- FFT motion feature (10s window by default)\n",
        "- combined anomaly scoring\n",
        "\n",
        "Outputs:\n",
        "- Annotated video\n",
        "- CSV log (per frame/person scores)\n"
      ],
      "metadata": {
        "id": "sABGmAx3BYg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Processing\n",
        "\n",
        "If you want face recognition, set faces_root to os.path.join(DATA_DIR, \"faces\").\n",
        "If not, leave it empty."
      ],
      "metadata": {
        "id": "S1Zkmv4DBhyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faces_root = os.path.join(DATA_DIR, \"faces\")  # optional (works only if face_recognition installed)\n",
        "\n",
        "# If you want to disable face ID entirely:\n",
        "# faces_root = \"\"\n",
        "\n",
        "!python -m src.process_video \\\n",
        "  --camera_id {cam_id} \\\n",
        "  --video_path \"{video_path}\" \\\n",
        "  --ae_model_path \"{ae_model_path}\" \\\n",
        "  --output_video \"{annotated_out}\" \\\n",
        "  --output_csv \"{csv_out}\" \\\n",
        "  --image_size 128 64 \\\n",
        "  --color_mode rgb \\\n",
        "  --yolo_model yolov8n.pt \\\n",
        "  --conf_thres 0.25 \\\n",
        "  --iou_thres 0.45 \\\n",
        "  --fft_window_seconds 10.0 \\\n",
        "  --w_recon 0.6 \\\n",
        "  --w_posture 0.25 \\\n",
        "  --w_fft 0.15 \\\n",
        "  --recon_max 0.1 \\\n",
        "  --anomaly_threshold 0.5 \\\n",
        "  --faces_root \"{faces_root}\"\n"
      ],
      "metadata": {
        "id": "qm1jM1dbBnDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview Results"
      ],
      "metadata": {
        "id": "6b_xiSheBpuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(csv_out)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "nm1x6FlWBqdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Summary of Anomalies"
      ],
      "metadata": {
        "id": "WPzp40SHBtoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frames flagged as anomaly\n",
        "anoms = df[df[\"is_anomaly\"] == 1]\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Anomaly rows:\", len(anoms))\n",
        "\n",
        "# show top 10 highest combined scores\n",
        "df.sort_values(\"combined_score\", ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "cDX7-yCuBuLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Annotated Video Inline"
      ],
      "metadata": {
        "id": "0svTHymWBxhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "Video(annotated_out, embed=True)\n"
      ],
      "metadata": {
        "id": "zi_QfE48Bys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes\n",
        "\n",
        "- The autoencoder is trained **only on normal data**, camera-by-camera.\n",
        "- The combined anomaly score uses:\n",
        "  - reconstruction error (primary signal)\n",
        "  - posture score (weak supporting feature)\n",
        "  - FFT motion score on the normalized vertical bbox center trajectory\n",
        "- Face recognition is optional and gracefully degrades to `\"unknown\"` if not installed.\n",
        "\n",
        "In production, anomaly flags could trigger a notification system (webhook/SMS/email),\n",
        "but this repository focuses on **modeling + scoring + reproducible inference**.\n"
      ],
      "metadata": {
        "id": "JVMAe38JB1di"
      }
    }
  ]
}